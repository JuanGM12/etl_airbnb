{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a70be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Importar seaborn de forma opcional\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    SEABORN_AVAILABLE = True\n",
    "    # Configuración para visualizaciones\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "except ImportError:\n",
    "    SEABORN_AVAILABLE = False\n",
    "    print(\"AVISO: Seaborn no está disponible. Las visualizaciones usarán matplotlib básico.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de pandas para mostrar más información\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "class EDAAirbnbWindows:\n",
    "    \"\"\"\n",
    "    Clase para realizar Análisis Exploratorio de Datos (EDA) de Airbnb.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extractor):\n",
    "        \"\"\"\n",
    "        Inicializa la clase EDA con un extractor de datos.\n",
    "        \n",
    "        Args:\n",
    "            extractor: Instancia de la clase Extraccion para obtener datos\n",
    "        \"\"\"\n",
    "        self.extractor = extractor\n",
    "        self.datos = {}\n",
    "        self.estadisticas_generales = {}\n",
    "        self.hallazgos = []\n",
    "        \n",
    "        print(\"INICIANDO ANALISIS EXPLORATORIO DE DATOS (EDA) DE AIRBNB...\")\n",
    "    \n",
    "    def cargar_datos_para_analisis(self, limite_por_coleccion: int = None, usar_muestreo_inteligente: bool = True):\n",
    "        \"\"\"\n",
    "        Carga los datos de todas las colecciones para el análisis.\n",
    "        \n",
    "        Args:\n",
    "            limite_por_coleccion (int, optional): Límite de registros por colección\n",
    "            usar_muestreo_inteligente (bool): Si True, usa límites inteligentes basados en el tamaño de cada colección\n",
    "        \"\"\"\n",
    "        print(\"\\nCARGANDO DATOS PARA ANALISIS...\")\n",
    "        \n",
    "        try:\n",
    "            # Primero obtener estadísticas de las colecciones\n",
    "            print(\"Analizando tamaño de las colecciones...\")\n",
    "            estadisticas = self.extractor.obtener_estadisticas_colecciones()\n",
    "            \n",
    "            # Extraer TODOS los datos disponibles (sin límites)\n",
    "            print(\"\\nExtrayendo TODOS los datos disponibles...\")\n",
    "            for coleccion, total_docs in estadisticas.items():\n",
    "                print(f\"   {coleccion}: {total_docs:,} docs -> Cargando TODOS los datos\")\n",
    "            \n",
    "            self.datos['listings'] = self.extractor.obtener_listings()\n",
    "            self.datos['reviews'] = self.extractor.obtener_reviews()\n",
    "            self.datos['calendar'] = self.extractor.obtener_calendar()\n",
    "            \n",
    "            print(f\"\\nDATOS CARGADOS EXITOSAMENTE:\")\n",
    "            for nombre, df in self.datos.items():\n",
    "                total_original = estadisticas.get(nombre, 0)\n",
    "                porcentaje = (len(df) / total_original * 100) if total_original > 0 else 100\n",
    "                print(f\"   {nombre}: {len(df):,} registros ({porcentaje:.1f}% del total), {len(df.columns)} columnas\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR al cargar datos: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def analisis_general_datos(self):\n",
    "        \"\"\"\n",
    "        Realiza análisis general de la estructura de los datos.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALISIS GENERAL DE DATOS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        analisis = {}\n",
    "        \n",
    "        for nombre_coleccion, df in self.datos.items():\n",
    "            print(f\"\\nAnalizando coleccion: {nombre_coleccion.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Información básica\n",
    "            info_basica = {\n",
    "                'registros': len(df),\n",
    "                'columnas': len(df.columns),\n",
    "                'memoria_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
    "            }\n",
    "            \n",
    "            print(f\"Registros: {info_basica['registros']:,}\")\n",
    "            print(f\"Columnas: {info_basica['columnas']}\")\n",
    "            print(f\"Memoria: {info_basica['memoria_mb']} MB\")\n",
    "            \n",
    "            # Mostrar primeras filas\n",
    "            print(f\"\\nPrimeras 3 filas:\")\n",
    "            print(df.head(3).to_string())\n",
    "            \n",
    "            # Información de tipos de datos\n",
    "            print(f\"\\nTipos de datos:\")\n",
    "            tipos_info = df.dtypes.value_counts()\n",
    "            for tipo, cantidad in tipos_info.items():\n",
    "                print(f\"   {tipo}: {cantidad} columnas\")\n",
    "            \n",
    "            # Columnas principales\n",
    "            print(f\"\\nColumnas principales:\")\n",
    "            columnas_principales = df.columns.tolist()[:10]\n",
    "            for i, col in enumerate(columnas_principales, 1):\n",
    "                print(f\"   {i:2d}. {col}\")\n",
    "            if len(df.columns) > 10:\n",
    "                print(f\"   ... y {len(df.columns) - 10} columnas más\")\n",
    "            \n",
    "            analisis[nombre_coleccion] = {\n",
    "                'info_basica': info_basica,\n",
    "                'tipos_datos': df.dtypes.to_dict(),\n",
    "                'columnas': df.columns.tolist()\n",
    "            }\n",
    "            \n",
    "            # Guardar hallazgo\n",
    "            self.hallazgos.append(f\"Coleccion {nombre_coleccion}: {info_basica['registros']:,} registros, {info_basica['columnas']} columnas\")\n",
    "        \n",
    "        self.estadisticas_generales = analisis\n",
    "        return analisis\n",
    "    \n",
    "    def analisis_calidad_datos(self):\n",
    "        \"\"\"\n",
    "        Analiza la calidad de los datos: nulos, duplicados y outliers.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALISIS DE CALIDAD DE DATOS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        calidad_analisis = {}\n",
    "        \n",
    "        for nombre_coleccion, df in self.datos.items():\n",
    "            print(f\"\\nAnalizando calidad: {nombre_coleccion.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Análisis de valores nulos\n",
    "            print(\"Analisis de valores nulos:\")\n",
    "            nulos_por_columna = df.isnull().sum()\n",
    "            porcentaje_nulos = (nulos_por_columna / len(df)) * 100\n",
    "            \n",
    "            columnas_con_nulos = nulos_por_columna[nulos_por_columna > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if len(columnas_con_nulos) > 0:\n",
    "                print(\"   Columnas con valores nulos:\")\n",
    "                for col, nulos in columnas_con_nulos.head(10).items():\n",
    "                    porcentaje = porcentaje_nulos[col]\n",
    "                    print(f\"   - {col}: {nulos:,} nulos ({porcentaje:.1f}%)\")\n",
    "            else:\n",
    "                print(\"   OK: No hay valores nulos\")\n",
    "            \n",
    "            # Análisis de duplicados\n",
    "            print(f\"\\nAnalisis de duplicados:\")\n",
    "            try:\n",
    "                # Primero intentar identificar columnas con listas o tipos no hasheables\n",
    "                columnas_problemas = []\n",
    "                for col in df.columns:\n",
    "                    try:\n",
    "                        # Intentar aplicar hash a una muestra de valores para detectar problemas\n",
    "                        muestra = df[col].dropna().head(10)\n",
    "                        for val in muestra:\n",
    "                            hash(val)  # Intentar hacer hash del valor\n",
    "                    except (TypeError, ValueError):\n",
    "                        columnas_problemas.append(col)\n",
    "                \n",
    "                # Si hay columnas problemáticas, excluirlas del análisis de duplicados\n",
    "                if columnas_problemas:\n",
    "                    print(f\"   Columnas con tipos no hasheables (listas/dicts): {columnas_problemas}\")\n",
    "                    columnas_validas = [col for col in df.columns if col not in columnas_problemas]\n",
    "                    if len(columnas_validas) > 0:\n",
    "                        duplicados = df[columnas_validas].duplicated().sum()\n",
    "                    else:\n",
    "                        print(\"   ERROR: Todas las columnas contienen tipos no hasheables\")\n",
    "                        duplicados = 0\n",
    "                else:\n",
    "                    duplicados = df.duplicated().sum()\n",
    "                \n",
    "                porcentaje_duplicados = (duplicados / len(df)) * 100\n",
    "                print(f\"   Duplicados: {duplicados:,} ({porcentaje_duplicados:.2f}%)\")\n",
    "                \n",
    "                if duplicados > 0:\n",
    "                    print(\"   ADVERTENCIA: Se recomienda revisar y posiblemente eliminar duplicados\")\n",
    "                    self.hallazgos.append(f\"Coleccion {nombre_coleccion}: {duplicados:,} duplicados encontrados\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ERROR durante el analisis de duplicados: {e}\")\n",
    "                print(\"   Continuando con el siguiente analisis...\")\n",
    "                duplicados = 0\n",
    "                porcentaje_duplicados = 0\n",
    "            \n",
    "            # Análisis de outliers para campos numéricos específicos\n",
    "            print(f\"\\nAnalisis de outliers:\")\n",
    "            campos_numericos = ['price', 'minimum_nights', 'availability_365', 'number_of_reviews']\n",
    "            campos_existentes = [col for col in campos_numericos if col in df.columns]\n",
    "            \n",
    "            for campo in campos_existentes:\n",
    "                if df[campo].dtype in ['int64', 'float64']:\n",
    "                    # Convertir a numérico si es posible\n",
    "                    serie_numerica = pd.to_numeric(df[campo], errors='coerce')\n",
    "                    \n",
    "                    if not serie_numerica.isna().all():\n",
    "                        Q1 = serie_numerica.quantile(0.25)\n",
    "                        Q3 = serie_numerica.quantile(0.75)\n",
    "                        IQR = Q3 - Q1\n",
    "                        limite_inferior = Q1 - 1.5 * IQR\n",
    "                        limite_superior = Q3 + 1.5 * IQR\n",
    "                        \n",
    "                        outliers = serie_numerica[(serie_numerica < limite_inferior) | (serie_numerica > limite_superior)]\n",
    "                        porcentaje_outliers = (len(outliers) / len(serie_numerica.dropna())) * 100\n",
    "                        \n",
    "                        print(f\"   - {campo}: {len(outliers):,} outliers ({porcentaje_outliers:.1f}%)\")\n",
    "                        print(f\"     Rango normal: [{limite_inferior:.2f}, {limite_superior:.2f}]\")\n",
    "                        \n",
    "                        if porcentaje_outliers > 5:\n",
    "                            self.hallazgos.append(f\"Campo {campo} en {nombre_coleccion}: {porcentaje_outliers:.1f}% de outliers\")\n",
    "            \n",
    "            # Resumen de calidad\n",
    "            calidad_resumen = {\n",
    "                'total_registros': len(df),\n",
    "                'columnas_con_nulos': len(columnas_con_nulos),\n",
    "                'total_nulos': nulos_por_columna.sum(),\n",
    "                'duplicados': duplicados,\n",
    "                'porcentaje_duplicados': porcentaje_duplicados\n",
    "            }\n",
    "            \n",
    "            calidad_analisis[nombre_coleccion] = calidad_resumen\n",
    "        \n",
    "        return calidad_analisis\n",
    "    \n",
    "    def analisis_transformaciones_potenciales(self):\n",
    "        \"\"\"\n",
    "        Analiza transformaciones potenciales necesarias en los datos.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALISIS DE TRANSFORMACIONES POTENCIALES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        transformaciones = {}\n",
    "        \n",
    "        for nombre_coleccion, df in self.datos.items():\n",
    "            print(f\"\\nAnalizando transformaciones: {nombre_coleccion.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Verificar campos anidados\n",
    "            print(\"Verificando campos anidados:\")\n",
    "            campos_anidados = []\n",
    "            for col in df.columns:\n",
    "                # Verificar si hay valores que parecen JSON o listas\n",
    "                muestra_valores = df[col].dropna().head(10)\n",
    "                for valor in muestra_valores:\n",
    "                    if isinstance(valor, str) and ('[' in valor or '{' in valor):\n",
    "                        campos_anidados.append(col)\n",
    "                        break\n",
    "            \n",
    "            if campos_anidados:\n",
    "                print(f\"   Campos potencialmente anidados: {campos_anidados}\")\n",
    "                self.hallazgos.append(f\"Coleccion {nombre_coleccion}: Campos anidados encontrados: {campos_anidados}\")\n",
    "            else:\n",
    "                print(\"   OK: No se detectaron campos anidados\")\n",
    "            \n",
    "            # Verificar campos de fecha\n",
    "            print(\"\\nVerificando campos de fecha:\")\n",
    "            campos_fecha = []\n",
    "            for col in df.columns:\n",
    "                if any(palabra in col.lower() for palabra in ['date', 'time', 'created', 'updated']):\n",
    "                    campos_fecha.append(col)\n",
    "            \n",
    "            if campos_fecha:\n",
    "                print(f\"   Campos de fecha detectados: {campos_fecha}\")\n",
    "                for campo in campos_fecha:\n",
    "                    if df[campo].dtype == 'object':\n",
    "                        print(f\"   - {campo}: Necesita conversión a datetime\")\n",
    "            else:\n",
    "                print(\"   OK: No se detectaron campos de fecha\")\n",
    "            \n",
    "            # Verificar campos de precio/moneda\n",
    "            print(\"\\nVerificando campos de precio:\")\n",
    "            campos_precio = []\n",
    "            for col in df.columns:\n",
    "                if any(palabra in col.lower() for palabra in ['price', 'cost', 'fee', 'rate']):\n",
    "                    campos_precio.append(col)\n",
    "            \n",
    "            if campos_precio:\n",
    "                print(f\"   Campos de precio detectados: {campos_precio}\")\n",
    "                for campo in campos_precio:\n",
    "                    if df[campo].dtype == 'object':\n",
    "                        print(f\"   - {campo}: Necesita limpieza y conversión numérica\")\n",
    "            else:\n",
    "                print(\"   OK: No se detectaron campos de precio\")\n",
    "            \n",
    "            # Verificar campos de texto que necesitan estandarización\n",
    "            print(\"\\nVerificando campos de texto:\")\n",
    "            campos_texto = df.select_dtypes(include=['object']).columns.tolist()\n",
    "            campos_texto_importantes = [col for col in campos_texto if any(\n",
    "                palabra in col.lower() for palabra in ['name', 'description', 'summary', 'title']\n",
    "            )]\n",
    "            \n",
    "            if campos_texto_importantes:\n",
    "                print(f\"   Campos de texto importantes: {campos_texto_importantes}\")\n",
    "        \n",
    "        return transformaciones\n",
    "    \n",
    "    def generar_visualizaciones(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaciones útiles para el análisis exploratorio.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERANDO VISUALIZACIONES UTILES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Crear directorio para imágenes si no existe\n",
    "        if not os.path.exists('imagenes'):\n",
    "            os.makedirs('imagenes')\n",
    "        \n",
    "        # Crear figura con análisis más útiles\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # Subplot 1: Tipos de habitación (más útil que distribución de precios)\n",
    "        if 'listings' in self.datos and 'room_type' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 1)\n",
    "            room_counts = self.datos['listings']['room_type'].value_counts()\n",
    "            colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "            bars = plt.bar(range(len(room_counts)), room_counts.values, \n",
    "                          color=colors[:len(room_counts)], edgecolor='black')\n",
    "            plt.title('Distribucion de Tipos de Habitacion', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Tipo de Habitacion')\n",
    "            plt.ylabel('Cantidad de Propiedades')\n",
    "            plt.xticks(range(len(room_counts)), room_counts.index, rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar valores en las barras\n",
    "            for bar, valor in zip(bars, room_counts.values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(room_counts.values)*0.01,\n",
    "                        f'{valor:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 2: Análisis de superhosts (más útil que boxplot)\n",
    "        if 'listings' in self.datos and 'host_is_superhost' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 2)\n",
    "            superhost_counts = self.datos['listings']['host_is_superhost'].value_counts()\n",
    "            colors = ['lightcoral', 'lightgreen']\n",
    "            bars = plt.bar(['No Superhost', 'Superhost'], superhost_counts.values, \n",
    "                          color=colors, edgecolor='black')\n",
    "            plt.title('Distribucion de Superhosts', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Cantidad de Hosts')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar porcentajes\n",
    "            total = superhost_counts.sum()\n",
    "            for bar, valor in zip(bars, superhost_counts.values):\n",
    "                porcentaje = (valor / total) * 100\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(superhost_counts.values)*0.01,\n",
    "                        f'{valor:,} ({porcentaje:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 3: Número de reviews por propiedad (más útil que disponibilidad)\n",
    "        if 'listings' in self.datos and 'number_of_reviews' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 3)\n",
    "            reviews_data = self.datos['listings']['number_of_reviews'].dropna()\n",
    "            # Filtrar outliers extremos para mejor visualización\n",
    "            Q1 = reviews_data.quantile(0.25)\n",
    "            Q3 = reviews_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            limite_superior = Q3 + 2 * IQR\n",
    "            reviews_filtrado = reviews_data[reviews_data <= limite_superior]\n",
    "            \n",
    "            plt.hist(reviews_filtrado, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "            plt.title('Distribucion de Reviews por Propiedad', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Numero de Reviews')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 4: Análisis de precios por tipo de habitación (más útil)\n",
    "        if 'listings' in self.datos and 'price' in self.datos['listings'].columns and 'room_type' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 4)\n",
    "            # Limpiar datos de precio\n",
    "            df_listings = self.datos['listings'].copy()\n",
    "            df_listings['price_clean'] = pd.to_numeric(\n",
    "                df_listings['price'].astype(str).str.replace('$', '').str.replace(',', ''), \n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            # Filtrar precios válidos y outliers extremos\n",
    "            precios_validos = df_listings['price_clean'].dropna()\n",
    "            Q1 = precios_validos.quantile(0.25)\n",
    "            Q3 = precios_validos.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            limite_superior = Q3 + 2 * IQR\n",
    "            precios_filtrado = precios_validos[precios_validos <= limite_superior]\n",
    "            \n",
    "            # Agrupar por tipo de habitación\n",
    "            df_filtrado = df_listings[df_listings['price_clean'] <= limite_superior]\n",
    "            precios_por_tipo = df_filtrado.groupby('room_type')['price_clean'].mean().sort_values(ascending=False)\n",
    "            \n",
    "            colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "            bars = plt.bar(range(len(precios_por_tipo)), precios_por_tipo.values, \n",
    "                          color=colors[:len(precios_por_tipo)], edgecolor='black')\n",
    "            plt.title('Precio Promedio por Tipo de Habitacion', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Tipo de Habitacion')\n",
    "            plt.ylabel('Precio Promedio ($)')\n",
    "            plt.xticks(range(len(precios_por_tipo)), precios_por_tipo.index, rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar valores en las barras\n",
    "            for bar, valor in zip(bars, precios_por_tipo.values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(precios_por_tipo.values)*0.01,\n",
    "                        f'${valor:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 5: Análisis de nulos por colección\n",
    "        plt.subplot(3, 3, 5)\n",
    "        colecciones = list(self.datos.keys())\n",
    "        nulos_por_coleccion = []\n",
    "        for nombre in colecciones:\n",
    "            nulos = self.datos[nombre].isnull().sum().sum()\n",
    "            nulos_por_coleccion.append(nulos)\n",
    "        \n",
    "        colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "        bars = plt.bar(colecciones, nulos_por_coleccion, \n",
    "                     color=colors[:len(colecciones)], edgecolor='black')\n",
    "        plt.title('Valores Nulos por Coleccion', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Coleccion')\n",
    "        plt.ylabel('Total de Nulos')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar, valor in zip(bars, nulos_por_coleccion):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(nulos_por_coleccion)*0.01,\n",
    "                    f'{valor:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 6: Tamaño de colecciones\n",
    "        plt.subplot(3, 3, 6)\n",
    "        tamanos = [len(df) for df in self.datos.values()]\n",
    "        bars = plt.bar(colecciones, tamanos, \n",
    "                     color=colors[:len(colecciones)], edgecolor='black')\n",
    "        plt.title('Tamano de Colecciones', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Coleccion')\n",
    "        plt.ylabel('Numero de Registros')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar, valor in zip(bars, tamanos):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(tamanos)*0.01,\n",
    "                    f'{valor:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 7: Análisis de disponibilidad por tipo de habitación (más útil que disponibilidad general)\n",
    "        if 'listings' in self.datos and 'availability_365' in self.datos['listings'].columns and 'room_type' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 7)\n",
    "            df_listings = self.datos['listings']\n",
    "            disponibilidad_por_tipo = df_listings.groupby('room_type')['availability_365'].mean().sort_values(ascending=False)\n",
    "            \n",
    "            colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "            bars = plt.bar(range(len(disponibilidad_por_tipo)), disponibilidad_por_tipo.values, \n",
    "                          color=colors[:len(disponibilidad_por_tipo)], edgecolor='black')\n",
    "            plt.title('Disponibilidad Promedio por Tipo', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Tipo de Habitacion')\n",
    "            plt.ylabel('Dias Disponibles (promedio)')\n",
    "            plt.xticks(range(len(disponibilidad_por_tipo)), disponibilidad_por_tipo.index, rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar valores en las barras\n",
    "            for bar, valor in zip(bars, disponibilidad_por_tipo.values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(disponibilidad_por_tipo.values)*0.01,\n",
    "                        f'{valor:.0f} dias', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Subplot 8: Análisis de reviews por mes (más útil)\n",
    "        if 'reviews' in self.datos and 'date' in self.datos['reviews'].columns:\n",
    "            plt.subplot(3, 3, 8)\n",
    "            # Convertir fecha a datetime\n",
    "            df_reviews = self.datos['reviews'].copy()\n",
    "            df_reviews['date'] = pd.to_datetime(df_reviews['date'], errors='coerce')\n",
    "            df_reviews = df_reviews.dropna(subset=['date'])\n",
    "            \n",
    "            # Agrupar por mes\n",
    "            df_reviews['mes'] = df_reviews['date'].dt.to_period('M')\n",
    "            reviews_por_mes = df_reviews['mes'].value_counts().sort_index()\n",
    "            \n",
    "            # Tomar solo los últimos 12 meses para mejor visualización\n",
    "            if len(reviews_por_mes) > 12:\n",
    "                reviews_por_mes = reviews_por_mes.tail(12)\n",
    "            \n",
    "            plt.plot(range(len(reviews_por_mes)), reviews_por_mes.values, \n",
    "                    marker='o', linewidth=2, markersize=6, color='darkblue')\n",
    "            plt.title('Reviews por Mes (Ultimos 12 meses)', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Mes')\n",
    "            plt.ylabel('Numero de Reviews')\n",
    "            plt.xticks(range(len(reviews_por_mes)), [str(x) for x in reviews_por_mes.index], rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 9: Análisis de propiedades por anfitrión (más útil)\n",
    "        if 'listings' in self.datos and 'host_id' in self.datos['listings'].columns:\n",
    "            plt.subplot(3, 3, 9)\n",
    "            propiedades_por_host = self.datos['listings']['host_id'].value_counts()\n",
    "            # Analizar distribución de propiedades por host\n",
    "            distribucion = propiedades_por_host.value_counts().sort_index()\n",
    "            \n",
    "            # Tomar solo los primeros 10 valores para mejor visualización\n",
    "            if len(distribucion) > 10:\n",
    "                distribucion = distribucion.head(10)\n",
    "            \n",
    "            colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink', \n",
    "                     'lightcyan', 'lightpink', 'lightgray', 'lightsteelblue', 'lightseagreen']\n",
    "            bars = plt.bar(range(len(distribucion)), distribucion.values, \n",
    "                          color=colors[:len(distribucion)], edgecolor='black')\n",
    "            plt.title('Distribucion de Propiedades por Host', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Numero de Propiedades por Host')\n",
    "            plt.ylabel('Cantidad de Hosts')\n",
    "            plt.xticks(range(len(distribucion)), distribucion.index, rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar valores en las barras\n",
    "            for bar, valor in zip(bars, distribucion.values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(distribucion.values)*0.01,\n",
    "                        f'{valor:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('imagenes/analisis_airbnb_util.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Visualizacion guardada: imagenes/analisis_airbnb_util.png\")\n",
    "    \n",
    "    def documentar_hallazgos(self):\n",
    "        \"\"\"\n",
    "        Documenta todos los hallazgos del análisis exploratorio.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DOCUMENTACION DE HALLAZGOS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        resumen_hallazgos = {\n",
    "            'fecha_analisis': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'colecciones_analizadas': list(self.datos.keys()),\n",
    "            'total_registros': sum(len(df) for df in self.datos.values()),\n",
    "            'hallazgos_principales': self.hallazgos\n",
    "        }\n",
    "        \n",
    "        print(\"HALLAZGOS PRINCIPALES:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for i, hallazgo in enumerate(self.hallazgos, 1):\n",
    "            print(f\"{i:2d}. {hallazgo}\")\n",
    "        \n",
    "        print(f\"\\nRESUMEN ESTADISTICO:\")\n",
    "        print(\"-\" * 30)\n",
    "        for nombre, df in self.datos.items():\n",
    "            print(f\"- {nombre.upper()}: {len(df):,} registros, {len(df.columns)} columnas\")\n",
    "        \n",
    "        # Guardar hallazgos en archivo\n",
    "        with open('hallazgos_eda_airbnb.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(resumen_hallazgos, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nHallazgos guardados en: hallazgos_eda_airbnb.json\")\n",
    "        \n",
    "        return resumen_hallazgos\n",
    "    \n",
    "    def ejecutar_analisis_completo(self, limite_por_coleccion: int = None, usar_muestreo_inteligente: bool = True):\n",
    "        \"\"\"\n",
    "        Ejecuta el análisis exploratorio completo.\n",
    "        \n",
    "        Args:\n",
    "            limite_por_coleccion (int, optional): Límite de registros por colección\n",
    "            usar_muestreo_inteligente (bool): Si True, usa límites inteligentes basados en el tamaño de cada colección\n",
    "        \"\"\"\n",
    "        print(\"INICIANDO ANALISIS EXPLORATORIO COMPLETO\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if usar_muestreo_inteligente:\n",
    "            print(\"Usando muestreo inteligente basado en el tamano de cada coleccion\")\n",
    "        else:\n",
    "            print(f\"Usando limite fijo: {limite_por_coleccion} registros por coleccion\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Cargar datos con muestreo inteligente\n",
    "            self.cargar_datos_para_analisis(limite_por_coleccion, usar_muestreo_inteligente)\n",
    "            \n",
    "            # 2. Análisis general\n",
    "            self.analisis_general_datos()\n",
    "            \n",
    "            # 3. Análisis de calidad\n",
    "            self.analisis_calidad_datos()\n",
    "            \n",
    "            # 4. Transformaciones potenciales\n",
    "            self.analisis_transformaciones_potenciales()\n",
    "            \n",
    "            # 5. Generar visualizaciones\n",
    "            self.generar_visualizaciones()\n",
    "            \n",
    "            # 6. Documentar hallazgos\n",
    "            self.documentar_hallazgos()\n",
    "            \n",
    "            print(\"\\nANALISIS EXPLORATORIO COMPLETADO EXITOSAMENTE!\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR durante el analisis: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Ejemplo de uso del análisis exploratorio de datos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import sys\n",
    "        sys.path.append('..')\n",
    "        from scr.extraccion import ExtraccionWindows\n",
    "          \n",
    "        # Crear instancia del extractor\n",
    "        print(\"Conectando a MongoDB...\")\n",
    "        extractor = ExtraccionWindows()\n",
    "        \n",
    "        # Crear instancia del EDA\n",
    "        eda = EDAAirbnbWindows(extractor)\n",
    "        \n",
    "        # Ejecutar análisis completo con muestreo inteligente\n",
    "        eda.ejecutar_analisis_completo(usar_muestreo_inteligente=True)\n",
    "        \n",
    "        # Cerrar conexión\n",
    "        extractor.cerrar_conexion()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR durante el analisis: {e}\")\n",
    "        print(\"Asegurate de que MongoDB este ejecutandose y que los datos esten cargados.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
